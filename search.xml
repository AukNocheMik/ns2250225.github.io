<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Django集成Github登陆]]></title>
    <url>%2F2017%2F10%2F30%2FDjango%E9%9B%86%E6%88%90Github%E7%99%BB%E9%99%86%2F</url>
    <content type="text"><![CDATA[开发环境 python3.5 django1.11.6 pip install social-auth-app-django 注册Guthub账号并获取ID和SECRET Django项目配置 打开settings.py，配置如下内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445...INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'social_django' # 添加第三方登录库app]# 设置登录的方式，这里是githubAUTHENTICATION_BACKENDS = ( 'social_core.backends.github.GithubOAuth2', 'django.contrib.auth.backends.ModelBackend',)SOCIAL_AUTH_URL_NAMESPACE = 'social'# 填写Github中获取到的KEY和SECRETSOCIAL_AUTH_GITHUB_KEY = '91c3a492abae2e8a9801'SOCIAL_AUTH_GITHUB_SECRET = '2400fedbc47b59fda64e291cf78b31d2af5ba2f9'SOCIAL_AUTH_GITHUB_USE_OPENID_AS_USERNAME = True# 登陆成功后的回调路由SOCIAL_AUTH_LOGIN_REDIRECT_URL = '/index'# 在TEMPLATES中的context_processors中增加两项TEMPLATES = [ &#123; 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates')] , 'APP_DIRS': True, 'OPTIONS': &#123; 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', 'social_django.context_processors.backends', # 增加这两项 'social_django.context_processors.login_redirect', ], &#125;, &#125;,] 打开urls.py，配置如下 1url(r'', include('social_django.urls', namespace='social')) 打开html模板文件，在需要登录的地方加入如下内容 1&lt;a href="&#123;% url "social:begin" "github" %&#125;"&gt;Github登陆&lt;/a&gt;]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ItemCF的游戏推荐系统]]></title>
    <url>%2F2017%2F10%2F29%2FItemCF%E7%9A%84%E6%B8%B8%E6%88%8F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[参考 《集体智慧编程》 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137# -*- coding: utf-8 -*-from math import sqrt# Returns a distance-based similarity score for person1 and person2def sim_distance(prefs, person1, person2): # Get the list of shared_items si=&#123;&#125; for item in prefs[person1]: if item in prefs[person2]: si[item]=1 # if they have no ratings in common, return 0 if len(si) == 0: return 0 # Add up the squares of all the differences sum_of_squares=sum([pow(prefs[person1][item]-prefs[person2][item], 2) for item in prefs[person1] if item in prefs[person2]]) return 1/(1+sum_of_squares)#计算皮尔逊相关度(1为完全正相关，-1为完成负相关)def sim_pearson(prefs, p1, p2): # Get the list of mutually rated items si = &#123;&#125; for item in prefs[p1]: if item in prefs[p2]: si[item] = 1 # if they are no ratings in common, return 0 if len(si) == 0: return 0 # Sum calculations n = len(si) # Sums of all the preferences sum1 = sum([prefs[p1][it] for it in si]) sum2 = sum([prefs[p2][it] for it in si]) # Sums of the squares sum1Sq = sum([pow(prefs[p1][it], 2) for it in si]) sum2Sq = sum([pow(prefs[p2][it], 2) for it in si]) # Sum of the products pSum = sum([prefs[p1][it] * prefs[p2][it] for it in si]) # Calculate r (Pearson score) num = pSum - (sum1 * sum2 / n) den = sqrt((sum1Sq - pow(sum1, 2) / n) * (sum2Sq - pow(sum2, 2) / n)) if den == 0: return 0 r = num / den return rdef topMatches(prefs, person, n=5, similarity=sim_pearson): scores=[(similarity(prefs, person, other), other) for other in prefs if other != person] scores.sort() scores.reverse() return scores[0: n]# 矩阵转置def transformPrefs(prefs): result = &#123;&#125; for person in prefs: for item in prefs[person]: result.setdefault(item, &#123;&#125;) # Flip item and person result[item][person] = prefs[person][item] return result# 计算item之间的相似度def calculateSimilarItems(prefs, n=10): # Create a dictionary of items showing which other items they # are most similar to. result = &#123;&#125; # Invert the preference matrix to be item-centric itemPrefs = transformPrefs(prefs) c = 0 for item in itemPrefs: # Status updates for large datasets c += 1 if c % 100 == 0: print("%d / %d" % (c,len(itemPrefs))) # Find the most similar items to this one scores=topMatches(itemPrefs, item, n=n, similarity=sim_distance) result[item] = scores return resultdef getRecommendedItems(prefs, itemMatch, user): userRatings = prefs[user] scores = &#123;&#125; totalSim = &#123;&#125; # Loop over items rated by this user for (item, rating) in userRatings.items( ): # Loop over items similar to this one for (similarity, item2) in itemMatch[item]: # Ignore if this user has already rated this item if item2 in userRatings: continue # Weighted sum of rating times similarity scores.setdefault(item2, 0) scores[item2] += similarity*rating # Sum of all the similarities totalSim.setdefault(item2, 0) totalSim[item2] += similarity # Divide each total score by total weighting to get an average rankings = [(score/totalSim[item], item) for item, score in scores.items()] # Return the rankings from highest to lowest rankings.sort() rankings.reverse() return rankingsif __name__ == "__main__": # 原始数据, 评分范围为1-5，对应不喜欢到非常喜欢 data = &#123; 'user1': &#123;"Dota2": 1, "League of Legends": 2, "Hearthstone": 3, "World of Warcraft": 4&#125;, 'user2': &#123;"Dota2": 1, "League of Legends": 3, "Hearthstone": 2, "World of Warcraft": 1&#125;, 'user3': &#123;"Dota2": 2, "League of Legends": 2, "Hearthstone": 1, "World of Warcraft": 4&#125; &#125; # 插入待推荐用户的数据(ns2250225) data['ns2250225'] = &#123;'Dota2': 1, 'League of Legends': 2&#125; # 计算所有游戏间的相似度 # 可以把计算好的结果用pickle模块持久化到本地 # 下次就直接加载，不用重新计算 itemSim = calculateSimilarItems(data) # 获取推荐的结果列表 recommend_item = getRecommendedItems(data, itemSim, "ns2250225") print(str(recommend_item)) # [(1.6363636363636362, 'Hearthstone'), (1.5185185185185186, 'World of Warcraft')]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Tornado的Ceph文件上传与下载及在线预览]]></title>
    <url>%2F2017%2F10%2F29%2F%E5%9F%BA%E4%BA%8ETornado%E7%9A%84Ceph%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E4%B8%8E%E4%B8%8B%E8%BD%BD%E5%8F%8A%E5%9C%A8%E7%BA%BF%E9%A2%84%E8%A7%88%2F</url>
    <content type="text"><![CDATA[概述 用tornado作为http服务器 在处理post/get请求的方法中，调用librados(python)接口实现读写 设置Content-disposition中为inline则在线浏览，attachment则下载 上传1234567891011121314151617181920212223242526272829303132333435363738394041#!/usr/bin/env python# -*- coding: UTF-8 -*-import tornado.ioloopimport tornado.webimport osimport radosclass UploadFileHandler(tornado.web.RequestHandler): def get(self): self.write(''' &lt;html&gt; &lt;head&gt;&lt;title&gt;Upload File&lt;/title&gt;&lt;/head&gt; &lt;body&gt; &lt;form action='file' enctype="multipart/form-data" method='post'&gt; &lt;input type='file' name='file'/&gt;&lt;br/&gt; &lt;input type='submit' value='submit'/&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; ''') def post(self): # 将上传文件写入到test池中 cluster = rados.Rados(conffile='/etc/ceph/ceph.conf') cluster.connect() ioctx = cluster.open_ioctx('test') file_metas=self.request.files['file'] for meta in file_metas: filename=meta['filename'] ioctx.write_full(filename, meta['body']) ioctx.close() self.write('finished!')app=tornado.web.Application([ (r'/file',UploadFileHandler),])if __name__ == '__main__': app.listen(3000) tornado.ioloop.IOLoop.instance().start() 下载12345678910111213141516171819202122232425262728293031323334353637383940414243#!/usr/bin/env python# -*- coding: UTF-8 -*-import tornado.ioloopimport tornado.webimport osimport radosclass DownloadFileHandler(tornado.web.RequestHandler): def get(self, filename): print('downloading... : ',filename) cluster = rados.Rados(conffile='/etc/ceph/ceph.conf') cluster.connect() ioctx = cluster.open_ioctx('test') self.set_header ('Content-Type', 'application/octet-stream') self.set_header ('Content-Disposition', 'attachment; filename='+filename) size, timestamp = ioctx.stat(filename) pagesize = 1024*1024 offset = 0 if pagesize &gt; size: pagesize = size while offset &lt; size: data = ioctx.read(filename, pagesize, offset) self.write(data) offset += pagesize ioctx.close() self.finish() app=tornado.web.Application([ (r'/download/(.*)', DownloadFileHandler),])if __name__ == '__main__': app.listen(3000) tornado.ioloop.IOLoop.instance().start() 在线预览1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#!/usr/bin/env python# -*- coding: UTF-8 -*-import tornado.ioloopimport tornado.webimport osimport radosimport mimetypesclass DownloadFileHandler(tornado.web.RequestHandler): def get(self, filename): print('downloading... : ',filename) cluster = rados.Rados(conffile='/etc/ceph/ceph.conf') cluster.connect() ioctx = cluster.open_ioctx('music') ctype = mimetypes.guess_type(filename)[0] self.set_header ('Content-Type', ctype) self.set_header ('Content-Disposition', 'inline; filename='+filename) size, timestamp = ioctx.stat(filename) pagesize = 1024*1024 offset = 0 if pagesize &gt; size: pagesize = size while offset &lt; size: data = ioctx.read(filename, pagesize, offset) self.write(data) offset += pagesize ioctx.close() self.finish()app=tornado.web.Application([ (r'/download/(.*)', DownloadFileHandler),])if __name__ == '__main__': app.listen(3000) tornado.ioloop.IOLoop.instance().start()]]></content>
      <categories>
        <category>Ceph</category>
      </categories>
      <tags>
        <tag>Ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph的Python接口]]></title>
    <url>%2F2017%2F10%2F29%2FCeph%E7%9A%84python%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[参考文章 ceph的python_api文档 连接ceph集群1234import radoscluster = rados.Rados(conffile='/etc/ceph/ceph.conf')cluster.connect() 创建与删除池12345678910111213# 列出可用的池pools = cluster.list_pools()for pool in pools: print pool# 创建池testcluster.create_pool('test')# 删除池cluster.delete_pool('test')# 判断是否存在一个池cluster.pool_exists('test') 列出池中所有的文件名123456789101112ioctx = cluster.open_ioctx('test')# 列出test池中的所有文件名object_iterator = ioctx.list_objects()while True : try : rados_object = object_iterator.next() print "Object contents = " + rados_object.key except StopIteration : breakioctx.close() 上传文件123456789101112# 连接到test池ioctx = cluster.open_ioctx('test')file_name = "yy.mp3"f = open("yy.mp3", "r")file_content = f.read()f.close()# 将文件写入池ioctx.write_full(file_name, file_content)ioctx.close() 下载文件1234567891011# 连接到test池ioctx = cluster.open_ioctx('test')f = open("yy.mp3", "w")# 将文件下载（写入）到本地f.write(ioctx.read("yy.mp3"))f.close()ioctx.close() 删除文件123456ioctx = cluster.open_ioctx('test')# 删除test池中的yy.mp3文件ioctx.remove_object("yy.mp3")ioctx.close() 断开ceph集群连接1cluster.shutdown()]]></content>
      <categories>
        <category>Ceph</category>
      </categories>
      <tags>
        <tag>Ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[妹子图图片全站抓取]]></title>
    <url>%2F2017%2F10%2F29%2F%E5%A6%B9%E5%AD%90%E5%9B%BE%E5%9B%BE%E7%89%87%E5%85%A8%E7%AB%99%E6%8A%93%E5%8F%96%2F</url>
    <content type="text"><![CDATA[环境 python3.5 scrapy1.4.0 代码 items.py 1234567# -*- coding: utf-8 -*-import scrapyclass MeiziSpiderItem(scrapy.Item): image_url = scrapy.Field() # 存放图片真实的URL refer_url = scrapy.Field() # 存放图片下载时对应的请求Refer spiders/meizi.py 123456789101112131415161718192021222324# -*- coding: utf-8 -*-from scrapy.spider import CrawlSpider, Rulefrom scrapy.linkextractors import LinkExtractorfrom MeiziSpider.items import MeiziSpiderItemclass MeiziSpider(CrawlSpider): name = 'meizi' allowed_domains = ['www.mzitu.com'] start_urls = ['http://www.mzitu.com/'] rules = ( Rule(LinkExtractor(allow=('http://www.mzitu.com/\d&#123;1,6&#125;',)), callback='parse_item', follow=True), Rule(LinkExtractor(allow=('http://www.mzitu.com/\d&#123;1,6&#125;/\d&#123;1,3&#125;',)), callback='parse_item', follow=True), ) def parse_item(self, response): img_item = MeiziSpiderItem() img_item['image_url'] = response.css(".main-image p a img::attr(src)").extract() img_item['refer_url'] = response.url yield img_item pipelines.py 1234567891011121314151617181920# -*- coding: utf-8 -*-from scrapy.contrib.pipeline.images import ImagesPipelinefrom scrapy.http import Requestclass MyImagesPipeline(ImagesPipeline): def get_media_requests(self, item, info): for image_url in item['image_url']: default_headers = &#123; 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36', 'referer': '&#123;&#125;'.format(item['refer_url']) &#125; yield Request(image_url, headers=default_headers) def item_completed(self, results, item, info): return item settings.py 12345678910...ROBOTSTXT_OBEY = FalseITEM_PIPELINES = &#123; 'MeiziSpider.pipelines.MyImagesPipeline': 5,&#125;IMAGES_URLS_FIELD ="image_url" #image_url是在items.py中配置的网络爬取得图片地址#配置保存本地的地址project_dir = os.path.abspath(os.path.dirname(__file__)) #获取当前爬虫项目的绝对路径IMAGES_STORE = os.path.join(project_dir, 'images') #组装新的图片路径... 效果 共抓取到3G多的图片，花费1.5小时]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
      </tags>
  </entry>
</search>
